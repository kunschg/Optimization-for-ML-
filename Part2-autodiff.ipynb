{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "np.random.seed(seed=42) # to have same results each time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import importlib\n",
    "import useful_functions\n",
    "importlib.reload(useful_functions)\n",
    "from useful_functions import generate_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: implement the relu function and its VJP in the format above. Using the finite difference equation (slide 13), make sure that the VJP is correct numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    x: an array\n",
    "\n",
    "  Returns:\n",
    "    - value of the function ReLU(x) \n",
    "    - function vjp to easily compute vjp of ReLU\n",
    "  \"\"\"\n",
    "  value = np.maximum(x,0) #by defintion of the ReLU function \n",
    "  \n",
    "  def vjp(u):\n",
    "    relu_derivative = (x > 0) * 1 #by definition of the derivative and ReLU function \n",
    "    vjp_wrt_x = np.multiply(u,relu_derivative) #using slides 27 of the course\n",
    "    return vjp_wrt_x,  \n",
    "\n",
    "  return value, vjp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create some functions to numerically check if the function defined above (and others ones later) are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vjp(f, x, u, eps=1e-3):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    f: a function returning a tuple of size 2: array and vjp\n",
    "    x: an array of size n \n",
    "    eps: numerical value (very small)\n",
    "    u: an array of size m \n",
    "\n",
    "  Returns:\n",
    "    numerical_vjp\n",
    "  \"\"\"\n",
    "  \n",
    "  def e(i): # to define each direction in the space\n",
    "    basis_vector = np.zeros(len(x))\n",
    "    basis_vector[i] = 1\n",
    "    return basis_vector\n",
    "  \n",
    "  Jacobian = np.zeros((len(f(x)[0]),len(x)))\n",
    "  for i in range (len(x)):\n",
    "    Jacobian[:,i] = (f(x + eps * e(i))[0] - f(x)[0]) / eps #finite difference\n",
    "  \n",
    "  return np.dot(Jacobian.T,u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numeric method gives for the VJP:\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -1.41025641 -1.23076923 -1.05128205 -0.87179487\n",
      " -0.69230769 -0.51282051 -0.33333333 -0.15384615  0.02564103  0.20512821\n",
      "  0.38461538  0.56410256  0.74358974  0.92307692  1.1025641   1.28205128\n",
      "  1.46153846  1.64102564  1.82051282  2.        ]\n"
     ]
    }
   ],
   "source": [
    "#test implementation \n",
    "# we define some values for x and u, they must same dimension because for ReLU space of inputs and outputs are the same\n",
    "x = np.linspace(start = -2, stop = 2, num = 40)\n",
    "u = np.linspace(start = -5, stop = 2, num = 40)\n",
    "\n",
    "implemented_vjp = relu(x)[1](u)[0]\n",
    "relu_numerical_vjp = test_vjp(relu, x, u, eps=1e-3)\n",
    "\n",
    "print(\"The numeric method gives for the VJP:\")\n",
    "print(relu_numerical_vjp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The implemented method gives for the VJP:\n",
      "[-0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.         -0.\n",
      " -0.         -0.         -1.41025641 -1.23076923 -1.05128205 -0.87179487\n",
      " -0.69230769 -0.51282051 -0.33333333 -0.15384615  0.02564103  0.20512821\n",
      "  0.38461538  0.56410256  0.74358974  0.92307692  1.1025641   1.28205128\n",
      "  1.46153846  1.64102564  1.82051282  2.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"The implemented method gives for the VJP:\")\n",
    "print(implemented_vjp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two methods give the same results indeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: reusing dot and relu, implement a 2-layer MLP with a relu activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we import dot from the google doc\n",
    "def dot(W, x):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    W: an a matrix of shape (n1,n)\n",
    "    x: an array of shape n \n",
    "\n",
    "  Returns:\n",
    "    - value of the function dot(W,x) \n",
    "    - function vjp to easily compute vjp of the dot product\n",
    "  \"\"\"\n",
    "  value = np.dot(W, x)\n",
    "\n",
    "  def vjp(u):\n",
    "    return np.outer(u, x), np.dot(W.T,u)\n",
    "\n",
    "  return value, vjp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp2(x, W1, W2):\n",
    "    \"\"\"\n",
    "    This function defines a MLP-2 architecture with ReLU function activation\n",
    "\n",
    "    Args:\n",
    "        x: an array of shape n \n",
    "        W1: an a matrix of shape (n1,n)\n",
    "        W2: an a matrix of shape (1,n1)\n",
    "\n",
    "    Returns:\n",
    "        - value of the function mlp2(x, W1, W2)\n",
    "        - function vjp to easily compute vjp of mlp2 archietcture \n",
    "    \"\"\"\n",
    "\n",
    "    x1 = dot(W1,x)[0] #1st linear layer \n",
    "    x2 = relu(x1)[0] #ReLU activation \n",
    "    x3 = dot(W2,x2)[0] #2nd linear layer\n",
    "    value = x3\n",
    "\n",
    "    def vjp(u):\n",
    "        \n",
    "        vjp_jacobian_dot_W2 = dot(W2,x2)[1](u) \n",
    "        vjp_wrt_W2 = vjp_jacobian_dot_W2[0] # we keep the 1st vjp, with respect to W2\n",
    "\n",
    "        vjp_jacobian_dot_W2_wrt_x2 = vjp_jacobian_dot_W2[1] # we keep the 2nd vjp, with respect to x2\n",
    "        vjp_jacobian_relu_wrt_x1 = relu(x1)[1](vjp_jacobian_dot_W2_wrt_x2)[0]\n",
    "\n",
    "        vjp_jacobian_dot_W1 =  dot(W1,x)[1](vjp_jacobian_relu_wrt_x1) \n",
    "        vjp_wrt_W1 = vjp_jacobian_dot_W1[0] # we keep the 1st vjp, with respect to W1\n",
    "        vjp_wrt_x = vjp_jacobian_dot_W1[1] # we keep the 2nd vjp, with respect to x\n",
    "        \n",
    "        return vjp_wrt_x,vjp_wrt_W1, vjp_wrt_W2\n",
    "\n",
    "    return value, vjp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: implement the squared loss VJP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_pred, y):\n",
    "    \"\"\"\n",
    "    This function defines squared-loss between 2 vectors\n",
    "\n",
    "    Args:\n",
    "        y_pred: a scalar\n",
    "        y: a scalar \n",
    "        \n",
    "    Returns:\n",
    "        - value of the function squared_loss(y_pred, y)\n",
    "        - function vjp to easily compute vjp of the squared loss\n",
    "    \"\"\"\n",
    "\n",
    "    residual = y_pred - y\n",
    "    value = 0.5 * np.sum(residual ** 2)\n",
    "\n",
    "    def vjp(u):\n",
    "        vjp_y_pred = np.multiply(residual,u)\n",
    "        vjp_y = -np.multiply(residual,u)\n",
    "        return vjp_y_pred, vjp_y\n",
    "    \n",
    "    # The code requires every output to be an array.\n",
    "    return np.array([value]), vjp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: implement the loss by composing mlp2 and squared_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, y, W1, W2):\n",
    "    \"\"\"\n",
    "    This function defines the loss, by combining previous function squared_loss and mlp2\n",
    "\n",
    "    Args:\n",
    "        x: an array of shape n \n",
    "        y: an array of shape n \n",
    "        W1: an a matrix of shape (n1,n)\n",
    "        W2: an a matrix of shape (1,n1)\n",
    "        \n",
    "    Returns:\n",
    "        - value of the function loss(x, y, W1, W2)\n",
    "        - function vjp to easily compute vjp of the loss\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = mlp2(x, W1, W2)[0]\n",
    "    value = squared_loss(y_pred, y)[0]\n",
    "\n",
    "    def vjp(u):\n",
    "        \n",
    "        vjp_squared_loss = squared_loss(y_pred, y)[1](u)\n",
    "        vjp_wrt_y = vjp_squared_loss[1] # we keep the vjp with respect to y \n",
    "\n",
    "        vjp_mlp2 = mlp2(x, W1, W2)[1](vjp_squared_loss[0])\n",
    "\n",
    "        vjp_wrt_W2 = vjp_mlp2[2] # we keep the vjp with respect to W2\n",
    "        vjp_wrt_W1 = vjp_mlp2[1] # we keep the vjp with respect to W1\n",
    "        vjp_wrt_x = vjp_mlp2[0] # we keep the vjp with respect to Wx\n",
    "        \n",
    "        return vjp_wrt_x, vjp_wrt_y, vjp_wrt_W1, vjp_wrt_W2\n",
    "\n",
    "    return value, vjp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's numerically test our architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the gradient for x coordinate \n",
    "\n",
    "def test_architecture_vjp(f,x,y,W1,W2,u, eps=1e-3): \n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        f: a function returning a tuple of size 2: array and vjp\n",
    "        x: an array of size n \n",
    "        eps: numerical value (very small)\n",
    "        u: an array of size m \n",
    "\n",
    "    Returns:\n",
    "        numerical_vjp\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def e(i): # to define each direction in the space\n",
    "        basis_vector = np.zeros(len(x))\n",
    "        basis_vector[i] = 1\n",
    "        return basis_vector\n",
    "  \n",
    "    Jacobian = np.zeros((len(f(x,y,W1,W2)[0]),len(x)))\n",
    "    for i in range (len(x)):\n",
    "        Jacobian[:,i] = (f(x + eps * e(i),y,W1,W2)[0] - f(x,y,W1,W2)[0]) / eps #finite difference\n",
    "  \n",
    "    return np.dot(Jacobian.T,u) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numeric method gives for the VJP:\n",
      "[16.04505928  9.91200029  5.58862923  4.74737583 12.21629471 13.86878034\n",
      " 12.30645815  7.59373855 11.96999804  9.19819874  7.38265823 11.13201473\n",
      "  8.60483474 17.65606679  7.53824422 13.24675525 11.19865447 10.18888991\n",
      "  8.32207167  9.29397093  7.73841677 13.31167587 13.22816866 11.3341041\n",
      "  7.63295551 13.95047329 13.93247314  6.50169319  4.40605946  9.02690877\n",
      " 10.16465822 15.82647781 14.90742599 14.32600886 11.50377891 16.81575735\n",
      " 13.76914738 16.01980159 15.46865053 14.18225387]\n"
     ]
    }
   ],
   "source": [
    "# we define some values for x and u, they must same dimension because for ReLU space of inputs and outputs are the same\n",
    "m = 5 #size \n",
    "k = 10 #size of the ouput vector for the MLP\n",
    "\n",
    "x = np.linspace(start = -2, stop = 2, num = 40) #size of the input vector\n",
    "y = np.linspace(start = -2, stop = 2, num = k) #size of the output vector\n",
    "u = np.random.rand(1) #size 1 for vjp because f will output a scalar\n",
    "\n",
    "W1 = np.random.rand(m,len(x))\n",
    "W2 = np.random.rand(k,m)\n",
    "\n",
    "relu_numerical_vjp = test_architecture_vjp(loss,x,y,W1,W2,u, eps=1e-3)\n",
    "implemented_vjp = loss(x, y, W1, W2)[1](u)[0]\n",
    "\n",
    "print(\"The numeric method gives for the VJP:\")\n",
    "print(relu_numerical_vjp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The implemented method gives for the VJP:\n",
      "[16.04259184  9.91108597  5.5883257   4.74714672 12.21490809 13.86684627\n",
      " 12.30487919  7.59311114 11.96865213  9.19741378  7.38206557 11.13082566\n",
      "  8.60410097 17.6530607   7.53767522 13.24511224 11.19743421 10.1879015\n",
      "  8.32130853  9.29309954  7.73779845 13.31003075 13.22653151 11.33281267\n",
      "  7.6323905  13.94866386 13.93061635  6.50128786  4.40587776  9.02601196\n",
      " 10.16367308 15.82409896 14.90530978 14.32393694 11.502476   16.81307263\n",
      " 13.7673902  16.01737479 15.46640677 14.18034267]\n"
     ]
    }
   ],
   "source": [
    "print(\"The implemented method gives for the VJP:\")\n",
    "print(implemented_vjp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 methods are really close but don't give exactly same results (because of numerical approximations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: implement an MLP with an arbitrary number of layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, *args):\n",
    "    \"\"\"\n",
    "    This function defines a MLP architecure with ReLU function activation\n",
    "\n",
    "    Args:\n",
    "        x: an array of shape n \n",
    "        *args: matrix of each layer. ATTENTION: dimension should match for dot product\n",
    "\n",
    "    Returns:\n",
    "        - value of the function mlp(x, *args)\n",
    "        - function vjp to easily compute vjp of mlp archietcture \n",
    "    \"\"\"\n",
    "    n_layers = len(args)\n",
    "    hidden_x = [] #store intermediate values along forward pass\n",
    "    input = x \n",
    "    \n",
    "    for layer in range(n_layers-1):\n",
    "        x_current_layer =  [0,0] \n",
    "\n",
    "        x = dot(args[layer],x)[0] #linear layer\n",
    "        x_current_layer[0] = x\n",
    "        \n",
    "        x = relu(x)[0] #ReLU activation \n",
    "        x_current_layer[1] = x\n",
    "        \n",
    "        hidden_x.append(x_current_layer) #store value both after linear transfo and relu activation\n",
    "    \n",
    "    value = dot(args[n_layers-1],hidden_x[n_layers-2][1])[0] #no activation on last layer\n",
    "\n",
    "    def vjp(u):\n",
    "        \n",
    "        list_of_vjp = []\n",
    "\n",
    "        #last layer\n",
    "        vjp_dot_last_layer = dot(args[n_layers-1],hidden_x[n_layers-2][1])[1](u) \n",
    "        vjp_wrt_weights = vjp_dot_last_layer[0] # we keep the 1st vjp, with respect to weights\n",
    "        vjp_wrt_x = vjp_dot_last_layer[1] \n",
    "\n",
    "        list_of_vjp.insert(0,vjp_wrt_weights)\n",
    "        \n",
    "        for layer in range(n_layers-2,0,-1):\n",
    "            vjp_relu_wrt_x = relu(hidden_x[layer][0])[1](vjp_wrt_x)[0] #backward relu\n",
    "            vjp_dot_current_layer = dot(args[layer],hidden_x[layer-1][1])[1](vjp_relu_wrt_x) #backward dot \n",
    "            vjp_wrt_weights = vjp_dot_current_layer[0] #keep weights\n",
    "            vjp_wrt_x = vjp_dot_current_layer[1] \n",
    "\n",
    "            list_of_vjp.insert(0,vjp_wrt_weights)\n",
    "        \n",
    "        #1st layer\n",
    "        vjp_relu_wrt_x = relu(hidden_x[0][0])[1](vjp_wrt_x)[0] #backward relu\n",
    "        vjp_dot_current_layer = dot(args[0],input)[1](vjp_relu_wrt_x) #backward dot \n",
    "        vjp_wrt_weights = vjp_dot_current_layer[0] #keep weights\n",
    "        vjp_wrt_x = vjp_dot_current_layer[1] \n",
    "\n",
    "        list_of_vjp.insert(0,vjp_wrt_weights)\n",
    "        list_of_vjp.insert(0,vjp_wrt_x)\n",
    "\n",
    "        return list_of_vjp\n",
    "\n",
    "    return value, vjp\n",
    "\n",
    "#should be ok: to check\n",
    "def loss_general_MLP(x, y, *args):\n",
    "    \"\"\"\n",
    "    This function defines the loss, by combining previous function squared_loss and mlp2\n",
    "\n",
    "    Args:\n",
    "        x: an array of shape n \n",
    "        y: an array of shape n \n",
    "        *args: matrix of each layer. ATTENTION: dimension should match for dot product\n",
    "        \n",
    "    Returns:\n",
    "        - value of the function loss(x, y, *args)\n",
    "        - function vjp to easily compute vjp of the loss\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = mlp(x, *args)[0]\n",
    "    value = squared_loss(y_pred, y)[0]\n",
    "\n",
    "    def vjp(u):\n",
    "        \n",
    "        vjp_squared_loss = squared_loss(y_pred, y)[1](u)\n",
    "        vjp_wrt_y = vjp_squared_loss[1] # we keep the vjp with respect to y \n",
    "\n",
    "        vjp_mlp = mlp(x, *args)[1](vjp_squared_loss[0])\n",
    "\n",
    "        list_of_vjp = []\n",
    "        list_of_vjp.append(vjp_mlp[0])# add vjp wrt to x \n",
    "        list_of_vjp.append(vjp_wrt_y)# add vjp wrt to y\n",
    "\n",
    "        list_of_vjp = list_of_vjp + vjp_mlp[1:] #add vjp wrt to weights \n",
    "        \n",
    "        return list_of_vjp\n",
    "\n",
    "    return value, vjp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: implement SGD to train your MLP on a dataset of your choice. Study the impact of depth (number of layers) and width (number of hidden units).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement our autodiff on a simple dataset with a neural network using mlp2.\n",
    "The dataset generated is an easy one with 200 samples and 2 features. A linear model is sufficient to capture it's complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate dataset with 2 features by row, with same distribution\n",
    "nsamples = 200\n",
    "range_uniform = 5 \n",
    "sigma = 2\n",
    "xtrain, ytrain = generate_dataset(nsamples, range_uniform, sigma)\n",
    "xtest, ytest = generate_dataset(nsamples, range_uniform, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimal training error: 3.099317795942237\n",
      "Minimal test error: 3.046711879571872\n"
     ]
    }
   ],
   "source": [
    "#test with mlp2\n",
    "learning_rate = 1e-4\n",
    "nepochs = 100\n",
    "\n",
    "input_dim = xtrain.shape[1]\n",
    "hidden_dim_1 = 2\n",
    "hidden_dim_2 = 2\n",
    "\n",
    "\n",
    "#weights initialisation\n",
    "W1 = np.random.normal(loc=0, scale=1./np.sqrt(input_dim),\n",
    "          size=(hidden_dim_1, input_dim))\n",
    "W2 = np.random.normal(loc=0, scale=1./np.sqrt(hidden_dim_1),\n",
    "          size=(hidden_dim_2, hidden_dim_1))\n",
    "\n",
    "training_loss, test_loss = [], []\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "\n",
    "    train_err = []\n",
    "    for n in range(nsamples):\n",
    "        random_index = np.random.randint(nsamples)\n",
    "        loss_train_it = loss(xtrain[random_index], ytrain[random_index], W1, W2)[0]\n",
    "        vector_for_starting_backward = 1 #because we use SGD, if batch would need of vector of len = batch_size\n",
    "        grad_W1 = loss(xtrain[random_index], ytrain[random_index], W1, W2)[1](vector_for_starting_backward)[2]\n",
    "        grad_W2  =  loss(xtrain[random_index], ytrain[random_index], W1, W2)[1](vector_for_starting_backward)[3]\n",
    "        W1 = W1 - learning_rate*grad_W1\n",
    "        W2 = W2 - learning_rate*grad_W2\n",
    "        train_err.append(loss_train_it)\n",
    "\n",
    "    test_err = []\n",
    "    for n in range(nsamples):\n",
    "        random_index = np.random.randint(nsamples)\n",
    "        loss_test_it = loss(xtrain[random_index], ytrain[random_index], W1, W2)[0]\n",
    "        test_err.append(loss_test_it)\n",
    "\n",
    "    training_loss.append(np.array(train_err).mean())\n",
    "    test_loss.append(np.array(test_err).mean())\n",
    "\n",
    "plt.plot(np.array(training_loss), label='training loss')\n",
    "plt.plot(np.array(test_loss), label='test loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Minimal training error: \" + str(min(training_loss)))\n",
    "print(\"Minimal test error: \" + str(min(test_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we generated is simple (linear regression on both coordinates on x to get y), so there is no surprise the mlp2 should be able to figure out \"good\" parameters. This is what we get here, both training loss and test loss goes towards 0 at what seems to be a geometric rate. Besides, we can observe the stochastic component of SGD on the long trail with small bump. \n",
    "As the dataset is simple, number of neurons is not so important: we achieve good results even with 2 neurons per layer.\n",
    "\n",
    "Nevertheless, interestingly the initialisation of weights play a crucial role. Indeed, even on such a simple dataset, a \"bad\" initialisation results in getting SGD stuck at local minima with poor performances. This illustrates the power of NN: they need to be given extra care to be efficient, either trough appropriate weight initialisation or through advanced SGD techniques (momentum, diagonal scalinl,...) or both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimal training error: 8.421000892277153\n",
      "Minimal test error: 8.433359211163365\n"
     ]
    }
   ],
   "source": [
    "#test with mlp general architecture\n",
    "learning_rate = 1e-4\n",
    "nepochs = 100\n",
    "\n",
    "#test with 4 layers\n",
    "input_dim = xtrain.shape[1]\n",
    "hidden_dim_1 = 2\n",
    "hidden_dim_2 = 3\n",
    "hidden_dim_3 = 4\n",
    "hidden_dim_4 = 5\n",
    "\n",
    "\n",
    "#weights initialisation\n",
    "W1 = np.random.normal(loc=0, scale=1./np.sqrt(input_dim),\n",
    "          size=(hidden_dim_1, input_dim))\n",
    "W2 = np.random.normal(loc=0, scale=1./np.sqrt(hidden_dim_1),\n",
    "          size=(hidden_dim_2, hidden_dim_1))\n",
    "W3 = np.random.normal(loc=0, scale=1./np.sqrt(hidden_dim_2),\n",
    "          size=(hidden_dim_3, hidden_dim_2))\n",
    "W4 = np.random.normal(loc=0, scale=1./np.sqrt(hidden_dim_3),\n",
    "          size=(hidden_dim_4, hidden_dim_3))\n",
    "\n",
    "training_loss, test_loss = [], []\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "\n",
    "    train_err = []\n",
    "    for n in range(nsamples):\n",
    "        random_index = np.random.randint(nsamples)\n",
    "        loss_train_it = loss_general_MLP(xtrain[random_index], ytrain[random_index], W1, W2, W3, W4)[0]\n",
    "        vector_for_starting_backward = 1 #because we use SGD, if batch would need of vector of len = batch_size\n",
    "        grad = loss_general_MLP(xtrain[random_index], ytrain[random_index], W1, W2,  W3, W4)[1](vector_for_starting_backward)\n",
    "        grad_W1 = grad[2]\n",
    "        grad_W2  =  grad[3]\n",
    "        grad_W3 = grad[4]\n",
    "        grad_W4  =  grad[5]\n",
    "        W1 = W1 - learning_rate*grad_W1\n",
    "        W2 = W2 - learning_rate*grad_W2\n",
    "        W3 = W3 - learning_rate*grad_W3\n",
    "        W4 = W4 - learning_rate*grad_W4\n",
    "\n",
    "        train_err.append(loss_train_it)\n",
    "\n",
    "    test_err = []\n",
    "    for n in range(nsamples):\n",
    "        random_index = np.random.randint(nsamples)\n",
    "        loss_test_it = loss_general_MLP(xtrain[random_index], ytrain[random_index], W1, W2, W3, W4)[0]\n",
    "        test_err.append(loss_test_it)\n",
    "\n",
    "    training_loss.append(np.array(train_err).mean())\n",
    "    test_loss.append(np.array(test_err).mean())\n",
    "\n",
    "plt.plot(np.array(training_loss), label='training loss')\n",
    "plt.plot(np.array(test_loss), label='test loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Minimal training error: \" + str(min(training_loss)))\n",
    "print(\"Minimal test error: \" + str(min(test_loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our dataset the number of layers shouldn't be too high. Indeed, the more complex the nework is the higer the risk it get stucks in local minima.\n",
    "Here with 3 hidden layers, we see we get stuck a few epochs in local minima but the networks managed to get out of it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "965fe59137d5334f034bf4d21dcfaab0895405927d7ec8dd4d8796ddf51dcb72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
